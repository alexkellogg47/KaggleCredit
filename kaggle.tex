\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps� with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}

\title{Kaggle my Baggle}
\author{Alex Kellogg, Bill DeRose, Jacob Fiksel, Lingge Li}
%\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle
\section{Introduction}
	Defaulting on loans played an integral role in the crumbling of the financial industry in the great recession of 2007-2008. The housing bubble was booming and banks wanted a share in the lucrative profits, so they engaged in misdirected and extremely risky lending practices (subprime mortgages, for example, which involve lending money to borrowers with lower credit ratings). As more and more banks engaged in these loans, the competition grew fierce (and the stakes were extremely high), which resulted in the institutions further relaxing their standards of credit worthiness. Banks were lending out way more than they could pay back. Finally, the subprime lending caught up with the banks, and all at once, a sea of people who were all classified as likely to default did indeed default. A few months later, the world faced the harshest recession since the 1930s. As a result of this default spiral, banks and regulators have started to pay special attention to estimating the likelihood of default (as well as its potential maximal costs). 
Enter Kaggle.com "Give Me Some Credit" competition. In this statistical modeling competition, we are provided with data from 150,000 people in 12 dimensions, including revolving utilization, monthly income, debt ratio, age, number of dependents, number of times late in paying loans, and number of real estate loans. With all this data, the goal is to train an algorithm to classify people as one of two types: serious delinquency in 2 years, or not – essentially, we are looking to asses the likelihood of default based on the factors mentioned above.

Data Cleanup and Scrutiny

The first, and arguably the most important, thing we did was examine and interpret the data. It did not take long to realize that there are a plethora of “NA” values, an issue we would certainly need to address in order to have better predictions (see Imputation Section). After a little more scrutiny in the form of creating box plots of each of the respective variables, we noticed a number of outliers. For example, someone was aged 0 and another was receiving several million dollars in monthly income (what industry is that in, please?). As a group, we decided to look into each of the variables and really think about the feasibility of the outlying values that we were observing. In doing so, we came to a couple of major realizations that helped boost our Kaggle score. 
The first key observation, and certainly the most significant of the three, was that 96\% of our data had a value of 0 in the field which we were to predict. Thus, each time we trained our data, the majority of the training was done on observations that would ultimately yield a value of zero (ie we could be 96\% correct by guessing 0’s for all values). We were noticing a large number of false negatives (predicted not to default, but did indeed default – the worst kind of error in this case), and decided that it would behoove us to train more of the data on people who did indeed default. Thus, we decided to weight the observations with a value of 1 in serious delinquency in 2 years more heavily than those with a 0. The way we did this was by creating a vector of probabilities, and assigning observations with a value of 1 a higher probability of being selected than those with a value of 0. In manipulating these weights, we were able to take subsets of our data (with resampling) with 40\% of this training data having defaulted. 
Next, we observed a dependency between debt ratio and monthly income (the former is a function of the latter). We had noticed, in both the box plots and a scatter plot of debt ratio vs monthly income, that low and NA values of monthly income were highly associated with large debt ratios. Our main concern was with the NA values – which produced a large percentage of the outliers, and so we proceeded to replace the debt ratios given to NA for every given monthly income value of NA. This got rid of a large number of the number of outliers.

\section{Clean Up}
	\subsection{Outliers}
	We begin our exploration of the data with summary statistics and plots. One of 
	the first anomolies we noticed was the presence of the values $96$ and $98$ 
	in the predictor variables. After doing some digging, we realized that 
	these quantitative values were used to code for qualitative values such as 
	``failed to answer or ``not applicable". We simply replaced these observations with
	\tt NA\rm~and let the imputation deal with the rest.
	
	Upon further inspection, we noted that some of the values for \tt RevolvingUtilizationOfUnsecuredLines
	\subsection{Imputation}
	Some individuals in the data set did not have values for certain values. Instead of ignoring blank values, and thus potentially ignoring values that could be key in classifying individuals, we decided to choose between two imputation methods: K-nearest neighbors imputation, and gradient boosting imputation. To evaluate the two methods, we focused on computation time and perceived accuracy of the methods. The K-Nearest Neighbor computes a distance matrix, and finds a speicifed number of closest observations to the individual you are imputing the data for. It then finds the mean of the variable you are imputing from those individuals. Unforunately, the algorithm in R was unable to compute a distance matrix for a data set of the size we are working with, which would force us to use much smaller subsets to impute the data, thus decreasing the accuracy of our imputations. Gradient boosting, the method that we used for our data, treats each missing value as a regression problem and uses boosting to create regression trees and predict the value as a weighted sum of the predictions. The weights are created to minimize a loss function, thus improving accuracy. While using regression to find missing values for our data set may not have been the best model, gradient boosting allowed us to use the training data to impute the missing values in the test data, and was significantly computationally less expensive than k-nearest neighbors imputation. 
	\subsection{Naive Bayes}
	A simple algorithm we used to contribute to our final classification was a naive bayes classifier. It uses a strong assumptation that each class contributes independently to the final classification, and thus is "naive." For each data point it computes the posterior probability of the individual being in a certain class by taking the product of the prior probabilites and the likelihood functions of the data. It classifies the individual into the group with the highest posterior probability. The advantages of this algorithm is that it is computationally fast, and does not require a large training set. 
	\subsection{Random Forest}
	\subsection{Boosting}
	\subsection{Genetic Algorithm}
\section{Conclusions}
	All in all, this project was extremely enjoyable for all of us, and presented us with the challenge of analysing a vast array of data, and figuring out the best way to classify the data set as defaulters and non defaulters. Although the more successful methods were addresed in the various sections of this paper, we learned a lot of valuable lessons in our numerous errors.
	One thing that comes to mind is our ever-changing perspective regarding the importance of the data. At first, we didn't pay much attention to the data we were dealing with, figuring that our classification methods would be able to sift through all the outliers and come to a reasonable estimate. However, after a quick set of box plots, we quickly changed our minds about the significance of our dataset, and went a little over the top by setting values of data that we found moderately sketchy to NA, or adjusting them down to "more reasonable" values. After testing our results, we learned that too much data manipulating was a bad thing, and we backed off a bit once more.
	In terms of implementation of the code, this project gave us a great opportunity to work with a particularly interesting data set (at least compared to our usual set of randomly generated values). As a result, we had to really take computational feasibility into account (running KNNImputation, for instance, was ruled out under this criteria), which was new to most of us. Furthermore, learning to handle imported libraries proved to be an interesting challenge: for the majority of the work in this class, we had been implementing our own algorithms in R, and working with pre-made packages required a new sort of problem solving that we had to adjust to in order to get our classifiers and various methods up and running.
	As a group, we are extremely pleased with our performance -- having jumped from around 600th place to 149th was an incredible leap made in just a couple of days! However, as we concluded coding, we realized that with more time, there were some additional things we would have liked to try. As previously mentioned, we believe that the values that we train our data on are extremely relevant to correct classification, and so one thing that we would have liked to focus a bit more energy on was imputation: although we are satisfied with how gmbImpute worked for us, when we scanned the imputed values of our data, we noticed some seemingly strange outputs. Clearly, we can't be sure that these imputed values were off, but when comparing them to KNNImpute (which, mind you, only could handle very small subsets of the data), we seemed to be able to better rationalize the KNNImpute data than the values we were using. 
	We would also have liked to spend more time dealing with the skewness of our data. As was mentioned earlier, 96\% of our data was from non-defaulters. In dealing with this, we explained that we chose to make a vector of probabilities to give more weight to observations of defaulters. However, we chose these probabilities arbitrarily; in order to even our data up, we usually trained our classifiers on data that was split 60-40 between defaulters and non-defaulters. Thus, if we had more time, we would be interested in solving for the optimal weights.
	That being said, we are very pleased with our results. Here's a link to our github repo, if you want to check out our code: https://github.com/billderose/KaggleCredit.

\end{document}  
