\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\usepackage{setspace}
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps� with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\doublespace
\title{Kaggle my Baggle}
\author{Alex Kellogg, Bill DeRose, Jacob Fiksel, Lingge Li}
%\date{}							% Activate to display a given date or no date


\begin{document}
\maketitle
\section{Introduction}
	 ~~~~Loan defaults played an integral role in the collapse of the financial industry in 2008. 
	In the midsts of a housing bubble, banks wanted their share of the profits and so engaged in
	 misdirected and extremely risky lending practices (subprime mortgages, for example, which involve lending money to borrowers with lower credit ratings). 
	 As more banks engaged in this type of lending, the competition for credit grew fierce and
	 institutions further relaxed their standards of credit worthiness. Banks were lending too 
	 much money to the wrong people. Lenders finally faced the consequences when many of 
	 those subprime loans were not repaid and in the ensuing months, the United States faced the 
	 worst recession since the 1930s. 
	 
	 As a result of this default spiral, banks and regulators have begun paying special attention to
	 the risk involved in lending. Enter Kaggle's "Give Me Some Credit" competition where
	 we are provided with data from 150,000 loans. Among the information provided about the 
	 borrowers we have availible the revolving utilization, monthly income, debt ratio, age, number
	 of dependents, number of times late in paying loans, and number of real estate loans. 
	 With data in hand, the task is then binary classification with the goal of predicting a borrowers'
	 probability of default. 

\section{Data Cleanup and Scrutiny}

The first, and arguably the most important, thing we did was examine and interpret the data. It did not take long to realize that there are a plethora of “NA” values, an issue we would certainly need to address in order to have better predictions (see Imputation Section). After a little more scrutiny in the form of creating box plots of each of the respective variables, we noticed a number of outliers. For example, someone was aged 0 and another was receiving several million dollars in monthly income (what industry is that in, please?). As a group, we decided to look into each of the variables and really think about the feasibility of the outlying values that we were observing. In doing so, we came to a couple of major realizations that helped boost our Kaggle score. 
The first key observation, and certainly the most significant of the three, was that 96\% of our data had a value of 0 in the field which we were to predict. Thus, each time we trained our data, the majority of the training was done on observations that would ultimately yield a value of zero (ie we could be 96\% correct by guessing 0’s for all values). We were noticing a large number of false negatives (predicted not to default, but did indeed default – the worst kind of error in this case), and decided that it would behoove us to train more of the data on people who did indeed default. Thus, we decided to weight the observations with a value of 1 in serious delinquency in 2 years more heavily than those with a 0. The way we did this was by creating a vector of probabilities, and assigning observations with a value of 1 a higher probability of being selected than those with a value of 0. In manipulating these weights, we were able to take subsets of our data (with resampling) with 40\% of this training data having defaulted. 
Next, we observed a dependency between debt ratio and monthly income (the former is a function of the latter). We had noticed, in both the box plots and a scatter plot of debt ratio vs monthly income, that low and NA values of monthly income were highly associated with large debt ratios. Our main concern was with the NA values – which produced a large percentage of the outliers, and so we proceeded to replace the debt ratios given to NA for every given monthly income value of NA. This got rid of a large number of the number of outliers.

\section{Clean Up}
	\subsection{Outliers}
	We begin our exploration of the data with summary statistics and plots. One of 
	the first anomalies we noticed was the presence of the values $96$ and $98$ 
	in the predictor variables. After doing some digging, we realized that 
	these quantitative values were used to code for qualitative values such as 
	``failed to answer or ``not applicable". We simply replaced these observations with
	\tt NA\rm~and let the imputation deal with the rest.
	\begin{center}
	\includegraphics[scale=0.5]{96}
	\end{center}
	
	
	
	\subsection{Imputation}
	Some individuals in the data set did not have values for certain values. 
	Instead of ignoring blank values, and thus potentially ignoring values that could be key in 
	classifying individuals, we decided to choose between two imputation methods: K-nearest 
	neighbors imputation, and gradient boosting imputation. To evaluate the two methods, we 
	focused on computation time and perceived accuracy of the methods. The K-Nearest Neighbor 
	computes a distance matrix, and finds a specified number of closest observations to the 
	individual you are imputing the data for. It then finds the mean of the variable you are imputing 
	from those individuals. Unfortunately, the algorithm in R was unable to compute a distance
	matrix for a data set of the size we are working with, which would force us to use much smaller
	subsets to impute the data, thus decreasing the accuracy of our imputations. Gradient
	boosting, the method that we used for our data, treats each missing value as a regression
	problem and uses boosting to create regression trees and predict the value as a weighted
	sum of the predictions. The weights are created to minimize a loss function, thus improving
	accuracy. While using regression to find missing values for our data set may not have been
	the best model, gradient boosting allowed us to use the training data to impute the missing 
	values in the test data, and was significantly computationally less expensive than k-nearest
	neighbors imputation. 
	
	\subsection{Skew}
	When we began working on the classification problem we were pleasantly surprised to see
	one of our classifiers correctly predict $93\%$ of the observations. Upon inspection,
	this number is not much better than guessing at random because at least $93\%$ of the
	data are all $0$'s.
	\begin{center}
	\includegraphics[scale=0.5]{skew}
	\end{center}  
	As we continued our work, we realized that our false positive rate was extremely high. We
	were not able to classify defaulters very well. We thought that this might be the result of 
	too few defaulters in the training observations. To remedy this we decided to selectively
	sample our training set so defaulters and non-defaulters were more equally represented:
	\begin{center}
	\includegraphics[scale=0.5]{less_skew}
	\end{center}
	As noted in our concluding remarks, we did not have time to pick an default to non-default
	ratio. Our results were the result of our belief that the two classes should be equally
	represented in the training data.
\section{Classifiers}
In general, we did not stick to a single classifier. We realized that for any classifier, there were myriad
parameters that we could try to tweak to get things exactly as we wanted. Instead, we 
opted to use the classifiers as they come``out-of-the-box" and combine results in an intelligent way. 
We wound up using a naive bayes classifier, a random forest, a support-vector machine, and 
boosted trees to predict defaults. We then fed those predictions to a genetic algorithm to do
the ``intelligent" part and figure out how to best weight each prediction.
	\subsection{Naive Bayes}
	A simple algorithm we used to contribute to our final classification was a naive bayes classifier. It uses a strong assumption that each class contributes independently to the final classification, and thus is ``naive." For each data point it computes the posterior probability of the individual being in a certain class by taking the product of the prior probabilities and the likelihood functions of the data. It classifies the individual into the group with the highest posterior probability. The advantages of this algorithm is that it is computationally fast, and does not require a large training set. 
	\subsection{Random Forest}
	Given the size of our data, we wanted a classifier that would scale well. From our lectures
	and research, we knew that random forests would perform well given the number of
	observations and predictor variables. In addition, random forests are not phased by the
	presence of missing data; this was a concern of ours early on before we had given though
	to imputation techniques. We also knew that random forests are very good at these
	types of problems and wanted to at least include them in our meta-ensemble.
	\subsection{Boosting}
	Our final classifier in the ensemble is itself an ensemble learner. We made use of the
	adaBoost algorithm which uses boosted regression trees. We used the \tt ada\rm~package
	whose \tt ada\rm~function we used right out of the box. We played around with the types of
	trees used to train the classifier. By default, the function uses random forests from the \tt rpart
	\rm~library. We tried stumps and a few other types of trees and ultimately settled
	on using trees with 16 terminal nodes. We did not make use of any cross-validation
	to make this decision, but instead relied on empirical results.
	\subsection{Genetic Algorithm}
\section{Conclusions}
	All in all, this project was extremely enjoyable for all of us, and presented us with the challenge of analysing a vast array of data, and figuring out the best way to classify the data set as defaulters and non defaulters. Although the more successful methods were addresed in the various sections of this paper, we learned a lot of valuable lessons in our numerous errors.
	One thing that comes to mind is our ever-changing perspective regarding the importance of the data. At first, we didn't pay much attention to the data we were dealing with, figuring that our classification methods would be able to sift through all the outliers and come to a reasonable estimate. However, after a quick set of box plots, we quickly changed our minds about the significance of our dataset, and went a little over the top by setting values of data that we found moderately sketchy to NA, or adjusting them down to "more reasonable" values. After testing our results, we learned that too much data manipulating was a bad thing, and we backed off a bit once more.
	In terms of implementation of the code, this project gave us a great opportunity to work with a particularly interesting data set (at least compared to our usual set of randomly generated values). As a result, we had to really take computational feasibility into account (running KNNImputation, for instance, was ruled out under this criteria), which was new to most of us. Furthermore, learning to handle imported libraries proved to be an interesting challenge: for the majority of the work in this class, we had been implementing our own algorithms in R, and working with pre-made packages required a new sort of problem solving that we had to adjust to in order to get our classifiers and various methods up and running.
	As a group, we are extremely pleased with our performance -- having jumped from around 600th place to 149th was an incredible leap made in just a couple of days! However, as we concluded coding, we realized that with more time, there were some additional things we would have liked to try. As previously mentioned, we believe that the values that we train our data on are extremely relevant to correct classification, and so one thing that we would have liked to focus a bit more energy on was imputation: although we are satisfied with how gmbImpute worked for us, when we scanned the imputed values of our data, we noticed some seemingly strange outputs. Clearly, we can't be sure that these imputed values were off, but when comparing them to KNNImpute (which, mind you, only could handle very small subsets of the data), we seemed to be able to better rationalize the KNNImpute data than the values we were using. 
	We would also have liked to spend more time dealing with the skewness of our data. As was mentioned earlier, 96\% of our data was from non-defaulters. In dealing with this, we explained that we chose to make a vector of probabilities to give more weight to observations of defaulters. However, we chose these probabilities arbitrarily; in order to even our data up, we usually trained our classifiers on data that was split 60-40 between defaulters and non-defaulters. Thus, if we had more time, we would be interested in solving for the optimal weights.
	That being said, we are very pleased with our results. Here's a link to our github repo, if you want to check out our code: https://github.com/billderose/KaggleCredit.

\end{document}  
